{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fuzzywuzzy import process,fuzz\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request,urlopen\n",
    "import requests\n",
    "import urllib.parse\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\asimf\\AppData\\Local\\Temp\\ipykernel_36228\\64001630.py:4: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"<!--\\[if lt IE 7\\]>\",\n",
      "C:\\Users\\asimf\\AppData\\Local\\Temp\\ipykernel_36228\\64001630.py:5: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"<!--\\[if IE 7\\]>\",\n",
      "C:\\Users\\asimf\\AppData\\Local\\Temp\\ipykernel_36228\\64001630.py:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"<!--\\[if IE 8\\]>\",\n",
      "C:\\Users\\asimf\\AppData\\Local\\Temp\\ipykernel_36228\\64001630.py:7: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"<!--\\[if gt IE 8\\]><!-->\",\n"
     ]
    }
   ],
   "source": [
    "def is_bot_page(html_content):\n",
    "    # Check for specific HTML comment patterns and classes related to bot detection\n",
    "    bot_detection_patterns = [\n",
    "        \"<!--\\[if lt IE 7\\]>\",\n",
    "        \"<!--\\[if IE 7\\]>\",\n",
    "        \"<!--\\[if IE 8\\]>\",\n",
    "        \"<!--\\[if gt IE 8\\]><!-->\",\n",
    "        \"class=\\\"a-no-js\\\"\",\n",
    "    ]\n",
    "    \n",
    "    # Check if any of the patterns are in the HTML content\n",
    "    for pattern in bot_detection_patterns:\n",
    "        if re.search(re.escape(pattern), html_content):\n",
    "            return True  # Bot detection mechanism detected\n",
    "\n",
    "    return False  # No bot detection mechanism detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.amazon.com/ref=nav_logo'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "def scroll(): # a function to slowly scroll down the page, this is useful when the page is not loaded properly and the wifi is slow\n",
    "    total_page_height = driver.execute_script(\"return document.body.scrollHeight\") #take height of the [age]\n",
    "    browser_window_height = driver.get_window_size(windowHandle='current')['height'] #window size\n",
    "    current_position = driver.execute_script('return window.pageYOffset') #current positon\n",
    "    while total_page_height - current_position > browser_window_height:\n",
    "        driver.execute_script(f\"window.scrollTo({current_position}, {browser_window_height + current_position});\")\n",
    "        current_position = driver.execute_script('return window.pageYOffset')\n",
    "        time.sleep(1) #this is an important step to scroll down slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_number(min_value=8, max_value=20):\n",
    "    \"\"\"Generate a random number between min_value and max_value, inclusive.\"\"\"\n",
    "    return random.randint(min_value, max_value)\n",
    "\n",
    "\n",
    "generate_random_number()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(\"xpath\",f'//*[@id=\"nav-hamburger-menu\"]/i').click()\n",
    "time.sleep(2)\n",
    "driver.find_element(\"xpath\",f'//*[@id=\"hmenu-content\"]/ul[1]/li[11]/a[1]/div').click()\n",
    "time.sleep(2.5)\n",
    "driver.find_element(\"xpath\",f'//*[@id=\"hmenu-content\"]/ul[1]/ul[1]/li[6]/a/i').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category=[]\n",
    "category_temp=driver.find_element(\"xpath\",\"/html/body/div[3]/div[2]/div/ul[13]/li[3]/a\")\n",
    "category.append(category_temp.text)\n",
    "category_temp.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory = []  # List to hold the extracted texts\n",
    "\n",
    "for index in range(5, 10):  # 15 is exclusive, so it actually goes up to 14\n",
    "    try:\n",
    "        # Construct the XPath with the current index\n",
    "        \n",
    "        xpath = f\"/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[2]/ul/span/span/li[{index}]/span/a/span\"\n",
    "        \n",
    "        # Wait for the element to be visible and get the element\n",
    "        element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath)))\n",
    "        subcategory.append(element.text)\n",
    "        time.sleep(2.3)\n",
    "        element.click()\n",
    "        time.sleep(5)\n",
    "        subcategory_page=driver.current_url\n",
    "        req = Request(subcategory_page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        time.sleep(5)\n",
    "        with requests.Session() as c:\n",
    "            soup = BeautifulSoup(webpage, 'html5lib')\n",
    "        driver.back()\n",
    "        time.sleep(generate_random_number())\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Extract the text and add it to the list\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with index {index}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory=[]\n",
    "try:  \n",
    "    xpath = f\"/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[2]/ul/span/span/li[6]/span/a/span\"\n",
    "        \n",
    "        # Wait for the element to be visible and get the element\n",
    "    element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath)))\n",
    "    subcategory.append(element.text)\n",
    "    time.sleep(2.3)\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "    subcategory_page=driver.current_url\n",
    "    req = Request(subcategory_page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    time.sleep(5)\n",
    "    with requests.Session() as c:\n",
    "      soup = BeautifulSoup(webpage, 'html5lib')\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred with index {index}: {e}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot detection page detected.\n"
     ]
    }
   ],
   "source": [
    "html_content = str(soup)\n",
    "if is_bot_page(html_content):\n",
    "    print(\"Bot detection page detected.\")\n",
    "else:\n",
    "    print(\"Normal page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details = {\"colors\": {}}\n",
    "\n",
    "# Assuming the driver has been initialized\n",
    "\n",
    "def get_colors():\n",
    "    \"\"\"\n",
    "    Attempts to find color options using different methods provided.\n",
    "    Returns a list of WebElement representing the color options.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First method to get colors\n",
    "        color_options = driver.find_elements(By.CSS_SELECTOR, \"img.imgSwatch\")\n",
    "        if color_options:\n",
    "            return color_options,1\n",
    "        else:\n",
    "            # Second method to get color if the first one fails\n",
    "            color_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CLASS_NAME, \"a-dropdown-prompt\")))\n",
    "            color_options = [color_element]  # Wrap the single element in a list for consistency\n",
    "            return color_options,2\n",
    "    except NoSuchElementException:\n",
    "        print(\"Color options not found.\")\n",
    "        return []\n",
    "\n",
    "def get_price():\n",
    "    try:\n",
    "\n",
    "        price_element = driver.find_element(By.XPATH, '//*[@id=\"corePriceDisplay_desktop_feature_div\"]/div[1]/span[3]') #first way of getting price\n",
    "        # Extract the price text\n",
    "        price = price_element.text\n",
    "        price = price.replace('\\n', '.')\n",
    "        print(price)\n",
    "        if (price==''): #second way of getting price\n",
    "            price_element = driver.find_element(By.XPATH, '//*[@id=\"corePriceDisplay_desktop_feature_div\"]/div[1]/span[2]/span[2]')\n",
    "\n",
    "    # Extract the price text\n",
    "\n",
    "            price = price_element.text\n",
    "            price = price.replace('\\n', '.')\n",
    "            print(\"Price:\", price)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            print('here')\n",
    "        # If the first XPath didn't find the element, try the new XPath\n",
    "            price_element = driver.find_element(By.XPATH, '//*[@id=\"corePrice_desktop\"]/div/table/tbody/tr/td[2]/span[1]/span[2]') #third way of getting price\n",
    "    # Extract the price text\n",
    "            price = price_element.text\n",
    "  \n",
    "\n",
    "        \n",
    "        # Print the extracted price\n",
    "            print(\"Price:\", price)\n",
    "            if price=='-': #fourth way of getting price\n",
    "                price_element1 = driver.find_element(By.XPATH, '//*[@id=\"corePrice_desktop\"]/div/table/tbody/tr/td[2]/span[1]/span[1]')\n",
    "\n",
    "                price_element = driver.find_element(By.XPATH, '//*[@id=\"corePrice_desktop\"]/div/table/tbody/tr/td[2]/span[1]/span[3]/span[2]')\n",
    "    # Extract the price text\n",
    "                price1=price_element1.text\n",
    "                price2 = price_element.text\n",
    "                price = price1 + ' - ' + price2\n",
    "                print(\"Price:\", price)\n",
    "    \n",
    "        except NoSuchElementException:\n",
    "            price='NA'\n",
    "            print(\"Price element not found.\")\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    return price  # Return 'NA' if all methods fail to find the price\n",
    "\n",
    "def get_picture_urls():\n",
    "    \"\"\"\n",
    "    Attempts to gather all picture URLs for the selected color by clicking through available buttons.\n",
    "    Returns a list of picture URLs.\n",
    "    \"\"\"\n",
    "    picture_urls = []\n",
    "    index = 1\n",
    "    index1=4\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            time.sleep(5)  # Sleep before attempting to find the next button to ensure the page has loaded\n",
    "            button_xpath = f'//*[@id=\"a-autoid-{index}\"]/span/input'\n",
    "            button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "            time.sleep(5)  # Sleep before clicking the button\n",
    "            #actions = ActionChains(driver)\n",
    "\n",
    "# Move to the element to hover over\n",
    "            #actions.move_to_element(button).perform()\n",
    "\n",
    "            button.click()\n",
    "\n",
    "            time.sleep(5)  # Wait a moment for any dynamic content to load\n",
    "            image_xpath = f'//*[@id=\"main-image-container\"]/ul/li[{index1}]/span/span/div/img'\n",
    "            \n",
    "            image_element = driver.find_element(By.XPATH, image_xpath)\n",
    "            image_url = image_element.get_attribute('src')\n",
    "            picture_urls.append(image_url)\n",
    "            index1+=1\n",
    "            index += 1\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"No more element options found.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return picture_urls\n",
    "\n",
    "\n",
    "# Main function to organize the scraping process\n",
    "def scrape_product_details():\n",
    "    #product_details = {\"colors\": {}}\n",
    "    sale_price = get_price()\n",
    "    color_options ,num= get_colors()\n",
    "    for color_option in color_options:\n",
    "        # Click on the color option to load its details\n",
    "        if num==1:\n",
    "            time.sleep(2)\n",
    "            color_option.click()\n",
    "            color_text = color_option.get_attribute(\"alt\")\n",
    "        else:\n",
    "            #print(color_options[0])\n",
    "            color_text = color_option.text.strip()\n",
    "        time.sleep(2)  # Wait for the page to load the color-specific details\n",
    "        picture_urls = get_picture_urls()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Save the details in the product details dictionary\n",
    "        product_details[\"colors\"][color_text] = {\n",
    "            \"images\": picture_urls,\n",
    "            \"price\": [{\"salePrice\": sale_price}]\n",
    "        }\n",
    "\n",
    "    #print(product_details)\n",
    "    return product_details\n",
    "\n",
    "\n",
    "#scrape_product_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colors': {'Black (12-pack)': {'images': [],\n",
       "   'price': [{'salePrice': '$15.42 - $69.99'}]},\n",
       "  'Black (6-pack)': {'images': [],\n",
       "   'price': [{'salePrice': '$15.42 - $69.99'}]},\n",
       "  'Black/Sport Grey/Charcoal (10-pack)': {'images': [],\n",
       "   'price': [{'salePrice': '$15.42 - $69.99'}]},\n",
       "  'Black/Sport Grey/Charcoal (5-pack)': {'images': [],\n",
       "   'price': [{'salePrice': '$15.42 - $69.99'}]},\n",
       "  'Black/Sport Grey/Military Green (5-pack)': {'images': [],\n",
       "   'price': [{'salePrice': '$15.42 - $69.99'}]}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed product 1: https://www.amazon.com/Amazon-Essentials-Full-Zip-Sweatshirt-X-Large/dp/B075JNL9S6/ref=sr_1_1\n",
      "Processed product 2: https://www.amazon.com/Pro-Club-Heavyweight-Fleece-Hoodie/dp/B01N45SX6P/ref=sr_1_2\n",
      "Finished processing 100 products.\n"
     ]
    }
   ],
   "source": [
    "product_urls = []  # List to hold the extracted URLs\n",
    "# Function to get all product URLs from the current page\n",
    "def get_product_urls():\n",
    "    divs = soup.find_all('div', class_='s-product-image-container')\n",
    "\n",
    "    for div in divs:\n",
    "    # Find the 'a' tag within the div\n",
    "        a_tag = div.find('a', class_='a-link-normal')\n",
    "        if a_tag:\n",
    "        # Extract the href attribute\n",
    "            href = a_tag['href']\n",
    "        \n",
    "        # Optional: Clean up the URL\n",
    "        # Assuming 'base_url' is the website's base URL, e.g., \"https://www.example.com\"\n",
    "            base_url = url\n",
    "            full_url = urllib.parse.urljoin(base_url, href)\n",
    "        \n",
    "        # Parse the URL to remove all query parameters for cleaning\n",
    "            parsed_url = urllib.parse.urlparse(full_url)\n",
    "            clean_url = urllib.parse.urlunparse(parsed_url._replace(query=\"\"))\n",
    "        \n",
    "        # Store the clean URL in the list\n",
    "            product_urls.append(clean_url)\n",
    "            \n",
    "        else:\n",
    "            print(\"No 'a' tag found within the div.\")\n",
    "    return product_urls\n",
    "\n",
    "# Function to navigate to the next page\n",
    "def go_to_next_page():\n",
    "    # Find and click the next page button. Update the selector as needed.\n",
    "    try:\n",
    "        scroll()\n",
    "        next_page_button = driver.find_element(By.CSS_SELECTOR, 'a.s-pagination-item.s-pagination-next.s-pagination-button.s-pagination-separator')\n",
    "\n",
    "        next_page_button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.staleness_of(next_page_button))\n",
    "        time.sleep(2) # Wait for the page to load completely\n",
    "    except NoSuchElementException:\n",
    "        print(\"Next page button not found.\")\n",
    "    except TimeoutException:\n",
    "        print(\"Page did not load in time.\")\n",
    "\n",
    "\n",
    "processed_count = 0\n",
    "max_products = 2 #100 per 1 subcategory\n",
    "while processed_count < max_products:\n",
    "    #main_product_urls.append(product_urls)\n",
    "    product_urls = get_product_urls()\n",
    "    \n",
    "    for url in product_urls:\n",
    "        if processed_count >= max_products:\n",
    "            break\n",
    "        #driver.get(url)\n",
    "        processed_count += 1\n",
    "        # Here, you can add more processing for each product page\n",
    "        print(f\"Processed product {processed_count}: {url}\")\n",
    "        # Go back to the product list page if needed, or manage tabs if you open each product in a new tab\n",
    "    \n",
    "    if processed_count < max_products:\n",
    "        print(\"Going to the next page.\")\n",
    "        go_to_next_page()\n",
    "\n",
    "print(\"Finished processing 100 products.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_elements_sequence(driver):\n",
    "    try:\n",
    "        # Click the first element by its XPath\n",
    "        first_element_xpath = '//*[@id=\"dropdown_selected_size_name\"]/span/span/span'\n",
    "        driver.find_element(By.XPATH, first_element_xpath).click()\n",
    "        time.sleep(2)  # Wait for animations or JS events\n",
    "        \n",
    "        # Wait for the second element to be clickable and then click it\n",
    "        second_element_xpath = '//*[@id=\"native_dropdown_selected_size_name_1\"]'\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, second_element_xpath)))\n",
    "        driver.find_element(By.XPATH, second_element_xpath).click()\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(\"Element not found.\")\n",
    "    except TimeoutException:\n",
    "        print(\"Element was not clickable within the given time.\")\n",
    "    except ElementClickInterceptedException:\n",
    "        print(\"Element was not clickable. It might be obscured or not interactable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko\",\n",
    "    \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 14_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "#random_user_agent = random.choice(user_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = Request(\n",
    "    \"https://www.amazon.com/Fruit-Loom-Mens-Tucked-T-Shirt/dp/B00W66LQFO/ref=sr_1_6?dib=eyJ2IjoiMSJ9.yldqNEOZt5xV-N8Ka1gonk_Hj8l7zd3T5rc1t3A5GtbjCs3S775b-3gznr2HhXVI-8tTvnHSUovle3Q3fiwhWeARwnIKI98v4Ggn1wWPO5-qnQ9tViHa_CfzMyLJVcVwEblKHyzmpEa8IME0csrCQcHRObzTM5Mnnd5xLyRVA0rwcBxtISGd_cazTyGFXChqv2eg5NFcsjgxJvHYgMdFlktbsHONFXaANnkBx_96UNGmWQDb17DiZ_fPQPtPXfZzAo3fQ_v-cswHXYAMlIZy665vek6SJjHqozF1BTxxruI.QGMhpwZL1LJap63mBgiaXJiVr4HGfvK3cG3E8maE3po&dib_tag=se&qid=1708360885&s=fashion-mens-intl-ship&sr=1-6\",\n",
    "    headers={'User-Agent': random.choice(user_agents)}\n",
    ")\n",
    "webpage = urlopen(req).read()\n",
    "time.sleep(generate_random_number())  # Wait for the page to load completely\n",
    "with requests.Session() as c:\n",
    "    soup = BeautifulSoup(webpage, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product title not found.\n"
     ]
    }
   ],
   "source": [
    "product_title_span = soup.find('span', id='productTitle')\n",
    "\n",
    "    # Check if the element is found to avoid AttributeError\n",
    "if product_title_span:\n",
    "    # Extract and strip the text to remove leading/trailing whitespaces\n",
    "    product_title = product_title_span.text.strip()\n",
    "    print(product_title)\n",
    "else:\n",
    "    print(\"Product title not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews():\n",
    "    def click_see_more_reviews(driver):\n",
    "        try:\n",
    "        # Locate the \"See more reviews\" button by its data-hook attribute\n",
    "            see_more_reviews_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[data-hook=\"see-all-reviews-link-foot\"]'))\n",
    "        )\n",
    "            see_more_reviews_button.click()\n",
    "            time.sleep(10)  # Wait for the page to load after clicking\n",
    "\n",
    "        # Return the current URL\n",
    "            return driver.current_url\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "    rev_url = click_see_more_reviews(driver)\n",
    "\n",
    "    if rev_url:\n",
    "    \n",
    "        req = Request(rev_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    })\n",
    "        webpage = urlopen(req).read()\n",
    "\n",
    "        with requests.Session() as c:\n",
    "            soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    \n",
    "    \n",
    "        # Store the reviews\n",
    "        product_reviews = []\n",
    "\n",
    "        while True:\n",
    "    # Fetch the content from the URL\n",
    "            time.sleep(8)\n",
    "            if len(product_reviews) == 30:\n",
    "                break\n",
    "            req = Request(rev_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        })\n",
    "            webpage = urlopen(req).read()\n",
    "\n",
    "            with requests.Session() as c:\n",
    "                soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    \n",
    "            review_blocks = soup.find_all(\"div\", class_=\"a-row\")\n",
    "\n",
    "            review_containers = soup.find_all(\"div\", class_=\"a-section celwidget\")\n",
    "\n",
    "            for review_container in review_containers:\n",
    "    # Extract the rating\n",
    "                rating_tag = review_container.find(\"i\", {\"data-hook\": \"review-star-rating\"})\n",
    "                rating = float(rating_tag.find(\"span\", class_=\"a-icon-alt\").text.split()[0]) if rating_tag else None\n",
    "    \n",
    "    # Extract the review title if present\n",
    "        #review_title_tag = review_container.find(\"a\", {\"data-hook\": \"review-title\"})\n",
    "        #review_title = review_title_tag.text.strip() if review_title_tag else None\n",
    "\n",
    "    # Extract the review body text\n",
    "                review_body_tag = review_container.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "    # Sometimes the actual review is nested within another span inside the review body\n",
    "                review_text_tag = review_body_tag.find(\"span\") if review_body_tag else None\n",
    "                review_text = review_text_tag.text.strip() if review_text_tag else None\n",
    "\n",
    "    # Combine review title and review text if both are present, else use what's available\n",
    "        #full_review_text = review_title + \" \" + review_text if review_title and review_text else review_title or review_text\n",
    "    \n",
    "                product_reviews.append({\"review\": review_text or None, \"rating\": rating})\n",
    "\n",
    "   \n",
    "    \n",
    "            try:\n",
    "            # Wait for the \"Next page\" link to be clickable\n",
    "                url = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, \"Next page\"))\n",
    "    )\n",
    "    \n",
    "    # Click on the \"Next page\" link\n",
    "                url.click()\n",
    "                time.sleep(15)\n",
    "                rev_url = driver.current_url\n",
    "            except (NoSuchElementException, TimeoutException) as e:\n",
    "                print(\"Next page link not found or not clickable.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                break\n",
    "    return product_reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_productdetails(product_url):\n",
    "    time.sleep(5)\n",
    "    driver.get(product_url)\n",
    "    req = Request(product_url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    })\n",
    "    webpage = urlopen(req).read()\n",
    "    time.sleep(generate_random_number())  # Wait for the page to load completely\n",
    "    with requests.Session() as c:\n",
    "        soup = BeautifulSoup(webpage, 'html5lib')\n",
    "    time.sleep(6)\n",
    "    #driver.get(product_urls)\n",
    "\n",
    "    # Optionally, wait for some time to let the Google page load or interact with it\n",
    "    #driver.implicitly_wait(5)\n",
    "    product_title_span = soup.find('span', id='productTitle')\n",
    "\n",
    "    # Check if the element is found to avoid AttributeError\n",
    "    if product_title_span:\n",
    "    # Extract and strip the text to remove leading/trailing whitespaces\n",
    "        product_title = product_title_span.text.strip()\n",
    "        print(product_title)\n",
    "    else:        \n",
    "        print(\"Product title not found.\")\n",
    "      \n",
    "    \n",
    "    subcategoryy=subcategory\n",
    "    Gender=[]\n",
    "    if any('Men\\'s' in category_word for category_word in subcategory):\n",
    "        Gender.append('Male')\n",
    "\n",
    "    a_tag = soup.find('a', id='bylineInfo')\n",
    "\n",
    "    if a_tag:\n",
    "        brand_text = a_tag.get_text(strip=True)\n",
    "        # Remove 'Visit the' if present\n",
    "        brand_name = brand_text.replace('Visit the ', '')\n",
    "        # Remove 'Brand:' and spaces\n",
    "        brand_name = brand_name.replace('Brand: ', '').strip()\n",
    "        print(brand_name)\n",
    "    else:\n",
    "        print(\"The specified element was not found.\")\n",
    "\n",
    "\n",
    "    click_elements_sequence(driver)\n",
    "\n",
    "    span_elements = soup.find_all('span', class_='a-list-item a-size-base a-color-base')\n",
    "\n",
    "    texts = [span.get_text(strip=True) for span in span_elements]\n",
    "    decription_text = ' '.join(texts)\n",
    "\n",
    "    decription_text\n",
    "\n",
    "    bold_spans = soup.find_all('span', class_='a-text-bold')\n",
    "\n",
    "    # Initialize ASIN value\n",
    "    asin_value = None\n",
    "    scroll()\n",
    "    # Loop through all bold spans and find the one that contains the text 'ASIN'\n",
    "    for bold_span in bold_spans:\n",
    "        if 'ASIN' in bold_span.text:\n",
    "            asin_span = bold_span.find_next_sibling()\n",
    "            if asin_span:\n",
    "                asin_value = asin_span.text.strip()\n",
    "                break\n",
    "        else:\n",
    "            print(\"ASIN value not found.\")\n",
    "\n",
    "    #productreview=reviews()\n",
    "\n",
    "    product_details = {\n",
    "        \"productUrl\": product_url,\n",
    "        \"productId\": asin_value,  # Placeholder, extract the actual ID\n",
    "        \"gender\": Gender[0],  # Determine based on the page or predefined\n",
    "        \"category\": category,  # Determine based on the page or predefined\n",
    "        \"description\":  decription_text,  # Extract the actual description\n",
    "        \"subCategory\": subcategoryy,  # Determine based on the page or predefined\n",
    "        \"productName\": product_title,\n",
    "        \"brandName\": brand_name\n",
    "       # \"Product reviews\": productreview\n",
    "    }\n",
    "\n",
    "    color_and_rating_info = scrape_product_details()\n",
    "    product_details.update(color_and_rating_info)\n",
    "\n",
    "    productreview=reviews()\n",
    "    product_details[\"Product reviews\"]=productreview\n",
    "\n",
    "    return product_details\n",
    "\n",
    "\n",
    "    # Assuming the scraping process is started\n",
    "    #product_details = {\"colors\": {}}\n",
    "    #product_details = scrape_product_details()\n",
    "    #product details looki like this {'colors': {'Black': {'images': ['https://m.media-amazon.com/images/I/81iKJ1CM21L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71T0oim9u7L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71mvdpjXVvL._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/81hAsGNV8+L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/611b7+trIUL._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71N3ZIN2GeL._AC_SL1500_.jpg'],\n",
    "   #'price': [{'salePrice': '$14.98'}]}\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_productdetails1(product_url):\n",
    "    time.sleep(5)\n",
    "    driver.get(product_url)\n",
    "  \n",
    "    time.sleep(6)\n",
    "    #driver.get(product_urls)\n",
    "\n",
    "    # Optionally, wait for some time to let the Google page load or interact with it\n",
    "    #driver.implicitly_wait(5)\n",
    "    product_title = driver.find_element(By.XPATH, '//*[@id=\"productTitle\"]').text\n",
    "\n",
    "    print(product_title)\n",
    "    time.sleep(6)\n",
    "    brand_text= driver.find_element(By.XPATH, '//*[@id=\"bylineInfo\"]').text\n",
    "    brand_name = brand_text.replace('Visit the ', '')\n",
    "    brand_name = brand_name.replace('Brand: ', '').strip()\n",
    "    print(brand_name)\n",
    "    \n",
    "    subcategoryy=subcategory\n",
    "    Gender=[]\n",
    "    if any('Men\\'s' in category_word for category_word in subcategory):\n",
    "        Gender.append('Male')\n",
    "    \n",
    "    click_elements_sequence(driver)\n",
    "    time.sleep(3)\n",
    "    texts = []\n",
    "\n",
    "# Find all span elements with the class 'a-list-item a-size-base a-color-base'\n",
    "    span_elements = driver.find_elements(By.CSS_SELECTOR, \"span.a-list-item.a-size-base.a-color-base\")\n",
    "\n",
    "# Loop through each span element and append its text content to the texts list\n",
    "    for span in span_elements:\n",
    "        texts.append(span.text.strip())\n",
    "\n",
    "    # Join all texts into a single string\n",
    "    decription_text = '.'.join(texts)\n",
    "\n",
    "    print(decription_text)\n",
    "    \n",
    "    try:\n",
    "    # Adjust the XPath to target the specific span element\n",
    "        asin_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"detailBullets_feature_div\"]/ul/li[6]/span/span[2]'))\n",
    "    )\n",
    "    # Extract and print the text\n",
    "        asin_text = asin_element.text\n",
    "        print(asin_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding asin element: {e}\")\n",
    "    \n",
    "\n",
    "    #productreview=reviews()\n",
    "\n",
    "    product_details = {\n",
    "        \"productUrl\": product_url,\n",
    "        \"productId\": asin_text,  # Placeholder, extract the actual ID\n",
    "        \"gender\": Gender[0],  # Determine based on the page or predefined\n",
    "        \"category\": category,  # Determine based on the page or predefined\n",
    "        \"description\":  decription_text,  # Extract the actual description\n",
    "        \"subCategory\": subcategoryy,  # Determine based on the page or predefined\n",
    "        \"productName\": product_title,\n",
    "         \"brandName\": brand_name\n",
    "       # \"Product reviews\": productreview\n",
    "    }\n",
    " \n",
    "    color_and_rating_info = scrape_product_details()\n",
    "    product_details.update(color_and_rating_info)\n",
    "    \n",
    "    productreview=reviews()\n",
    "    product_details[\"Product reviews\"]=productreview\n",
    "\n",
    "    return product_details\n",
    "\n",
    "\n",
    "    # Assuming the scraping process is started\n",
    "    #product_details = {\"colors\": {}}\n",
    "    #product_details = scrape_product_details()\n",
    "    #product details looki like this {'colors': {'Black': {'images': ['https://m.media-amazon.com/images/I/81iKJ1CM21L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71T0oim9u7L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71mvdpjXVvL._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/81hAsGNV8+L._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/611b7+trIUL._AC_SL1500_.jpg',\n",
    "    #'https://m.media-amazon.com/images/I/71N3ZIN2GeL._AC_SL1500_.jpg'],\n",
    "   #'price': [{'salePrice': '$14.98'}]}\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Essentials Men's Full-Zip Hooded Fleece Sweatshirt (Available in Big & Tall)\n",
      "Amazon Essentials Store\n",
      "REGULAR FIT: Comfortable, easy fit through the shoulders, chest, and waist.BRUSHED FLEECE: 8.3 oz soft brushed-back cotton-polyester blend fleece for staying cozy and keeping warm...\n",
      "B07Q1LC2K2\n",
      "\n",
      "Price: $16.40\n",
      "No more element options found.\n",
      "Pro Club Men's Heavyweight Full Zip Fleece Hoodie\n",
      "Pro Club Store\n",
      "Heavyweight fleece hoodie with front zipper closure and two front pockets.Constructed with soft and durable 13oz 60 perercent Ring-Spun Cotton, 40 percent Polyester fleece.Relaxed fit for superior comfort. Famous for its premium thickness and weight.Double-needle coverseamed neck, armholes, and waistband. Double-ply self fabric lined hood.\n",
      "B01N574BBH\n",
      "here\n",
      "Price element not found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n",
      "No more element options found.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\socket.py:837\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    836\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 837\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m all_product_details \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, product_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(product_urls, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     details \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_productdetails1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     all_product_details[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct details\u001b[39m\u001b[38;5;124m\"\u001b[39m: details}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Saving the scraped data to a JSON file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#with open('product_details.json', 'w') as outfile:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#    json.dump(all_product_details, outfile, indent=4)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#driver.quit()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 69\u001b[0m, in \u001b[0;36mscrape_productdetails1\u001b[1;34m(product_url)\u001b[0m\n\u001b[0;32m     66\u001b[0m color_and_rating_info \u001b[38;5;241m=\u001b[39m scrape_product_details()\n\u001b[0;32m     67\u001b[0m product_details\u001b[38;5;241m.\u001b[39mupdate(color_and_rating_info)\n\u001b[1;32m---> 69\u001b[0m productreview\u001b[38;5;241m=\u001b[39m\u001b[43mreviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m product_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mproductreview\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m product_details\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mreviews\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rev_url:\n\u001b[0;32m     20\u001b[0m     req \u001b[38;5;241m=\u001b[39m Request(rev_url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m })\n\u001b[1;32m---> 22\u001b[0m     webpage \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m c:\n\u001b[0;32m     25\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(webpage, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1327\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1325\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1373\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1322\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1081\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1079\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1081\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1084\u001b[0m \n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1025\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1461\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1461\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m   1464\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:991\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[0;32m    990\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m--> 991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[1;32m--> 844\u001b[0m         \u001b[43mexceptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[0;32m    845\u001b[0m     exceptions\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_product_details = {}\n",
    "\n",
    "for index, product_url in enumerate(product_urls, start=0):\n",
    "    details = scrape_productdetails1(product_url)\n",
    "    all_product_details[f\"Product {index}\"] = {\"product details\": details}\n",
    "\n",
    "# Saving the scraped data to a JSON file\n",
    "#with open('product_details.json', 'w') as outfile:\n",
    "#    json.dump(all_product_details, outfile, indent=4)\n",
    "\n",
    "#driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_product_details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mall_product_details\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_product_details' is not defined"
     ]
    }
   ],
   "source": [
    "all_product_details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
